.. _install_sandboxes_opentelemetry:

OpenTelemetry tracing
=====================

.. sidebar:: Requirements

   .. include:: _include/docker-env-setup-link.rst

   :ref:`curl <start_sandboxes_setup_curl>`
        Used to make ``HTTP`` requests.

The OpenTelemetry tracing sandbox demonstrates Envoy's :ref:`request tracing <arch_overview_tracing>`
capabilities using `OpenTelemetry <https://opentelemetry.io/>`_ as the tracing provider, and how it
correlates :ref:`access logs <arch_overview_access_logs>` with the tracing data.

In this example, 2 backend services are provided:

- ``service-1``
- ``service-2``

3 Envoy proxies are also provided to route requests to them:

- ``envoy-front-proxy`` (:download:`envoy-front-proxy.yaml <_include/opentelemetry/envoy-front-proxy.yaml>`)
- ``envoy-1`` (:download:`envoy-1.yaml <_include/opentelemetry/envoy-1.yaml>`)
- ``envoy-2`` (:download:`envoy-2.yaml <_include/opentelemetry/envoy-2.yaml>`)

Of these services, only the Envoy ``front-proxy`` service is exposed outside of the
:download:`composition <_include/opentelemetry/docker-compose.yaml>`, on port ``10000``.

For ``service-1``, requests are routed based on the request path ``trace/1``, as follows:

    User -> Envoy(``envoy-front-proxy``) -> Envoy(``envoy-1``) -> ``service-1``

For ``service-2``, requests are routed based on the request path ``trace/2`` as follows:

    User -> Envoy(``envoy-front-proxy``) -> Envoy(``envoy-1``) -> Envoy(``envoy-2``) -> ``service-2``

All Envoy proxies are configured to collect request traces, as can be seen in their configurations,
propagating the spans (parent/child/shared context) generated by the OpenTelemetry tracer to a OpenTelemetry cluster.

Each span records the latency of upstream API calls as well as information
needed to correlate the span with other related spans (e.g., the trace ID).

The OpenTelemetry collector provides a web UI for viewing the collected traces on port ``55679``.

Step 1: Build the sandbox
*************************

Change directory to ``examples/opentelemetry`` in the Envoy repository.

To build this sandbox example, and start the example services run the following commands:

.. code-block:: console

    $ pwd
    envoy/examples/opentelemetry
    $ docker compose pull
    $ docker compose up --build -d
    $ docker compose ps

                  Name                             Command                 State                          Ports
    -------------------------------------------------------------------------------------------------------------------------------
    opentelemetry_envoy-1_1             /docker-entrypoint.sh /usr ... Up             10000/tcp
    opentelemetry_envoy-2_1             /docker-entrypoint.sh /usr ... Up             10000/tcp
    opentelemetry_envoy-front-proxy_1   /docker-entrypoint.sh /usr ... Up             0.0.0.0:10000->10000/tcp
    opentelemetry_opentelemetry_1       /otelcol --config=/etc/ote ... Up (healthy)   4317/tcp, 55678/tcp, 0.0.0.0:55679->55679/tcp
    opentelemetry_service-1_1           python3 /code/service.py       Up (healthy)
    opentelemetry_service-2_1           python3 /code/service.py       Up (healthy)

Step 2: Make a request to ``service-1``
***************************************

Now send a request to ``service-1``, by calling http://localhost:10000/trace/1.

This will be routed via 2 of the Envoy proxies:

- ``front-proxy``
- ``envoy-1``

.. code-block:: console

    $ curl localhost:10000/trace/1
    Hello from behind Envoy (service 1)!

Step 3: Make a request to ``service-2``
***************************************

Now send a request to ``service-2``, by calling http://localhost:10000/trace/2.

This will be routed via all 3 of the Envoy proxies:

- ``front-proxy``
- ``envoy-1``
- ``envoy-2``

.. code-block:: console

    $ curl localhost:10000/trace/2
    Hello from behind Envoy (service 2)!

Step 4: View the traces in OpenTelemetry UI
*******************************************

Point your browser to http://localhost:55679/debug/tracez.

You should see the OpenTelemetry dashboard.

.. image:: /start/sandboxes/_static/opentelemetry-ui.png

In the ``Latency Samples`` of ``opentelemetry.proto.collector.trace.v1.TraceService/Export`` you can explore the traces by clicking any value of
``[>0s][>10µs][>100µs][>1ms][>10ms][>100ms][>1s][>10s][>1m40s]``.

.. image:: /start/sandboxes/_static/opentelemetry-ui-traces.png

.. seealso::

   :ref:`Request tracing <arch_overview_tracing>`
      Learn more about using Envoy's request tracing.

   `OpenTelemetry <https://opentelemetry.io/>`_
      OpenTelemetry tracing website.

Step 5: View the access logs in the OpenTelemetry collector output
******************************************************************

Check the spans Docker-compose logs:

.. code-block:: console
   :linenos:
   :emphasize-lines: 9,11

    opentelemetry-opentelemetry-1      | 2024-05-02T15:08:02.068Z   info    ResourceSpans #0
    opentelemetry-opentelemetry-1      | Resource SchemaURL: 
    opentelemetry-opentelemetry-1      | Resource attributes:
    opentelemetry-opentelemetry-1      |      -> service.name: Str(envoy-1)
    opentelemetry-opentelemetry-1      | ScopeSpans #0
    opentelemetry-opentelemetry-1      | ScopeSpans SchemaURL: 
    opentelemetry-opentelemetry-1      | InstrumentationScope  
    opentelemetry-opentelemetry-1      | Span #0
    opentelemetry-opentelemetry-1      |     Trace ID       : 832518ad68578df8182cd76847693a93
    opentelemetry-opentelemetry-1      |     Parent ID      : 869247f07978fa64
    opentelemetry-opentelemetry-1      |     ID             : 8af519328fd9ec0c
    opentelemetry-opentelemetry-1      |     Name           : egress localhost:10000
    opentelemetry-opentelemetry-1      |     Kind           : Client
    opentelemetry-opentelemetry-1      |     Start time     : 2024-05-02 15:08:01.190174 +0000 UTC
    opentelemetry-opentelemetry-1      |     End time       : 2024-05-02 15:08:01.199308 +0000 UTC
    opentelemetry-opentelemetry-1      |     Status code    : Unset
    opentelemetry-opentelemetry-1      |     Status message : 
    opentelemetry-opentelemetry-1      | Attributes:
    opentelemetry-opentelemetry-1      |      -> node_id: Str()
    opentelemetry-opentelemetry-1      |      -> zone: Str()
    opentelemetry-opentelemetry-1      |      -> guid:x-request-id: Str(84b13eee-db19-9fa4-b34d-b9a2d43b1064)
    opentelemetry-opentelemetry-1      |      -> http.url: Str(http://localhost:10000/trace/2)
    opentelemetry-opentelemetry-1      |      -> http.method: Str(GET)
    opentelemetry-opentelemetry-1      |      -> downstream_cluster: Str(-)
    opentelemetry-opentelemetry-1      |      -> user_agent: Str(curl/8.1.2)
    opentelemetry-opentelemetry-1      |      -> http.protocol: Str(HTTP/1.1)
    opentelemetry-opentelemetry-1      |      -> peer.address: Str(172.30.0.7)
    opentelemetry-opentelemetry-1      |      -> request_size: Str(0)
    opentelemetry-opentelemetry-1      |      -> response_size: Str(37)
    opentelemetry-opentelemetry-1      |      -> component: Str(proxy)
    opentelemetry-opentelemetry-1      |      -> upstream_cluster: Str(envoy_cluster2)
    opentelemetry-opentelemetry-1      |      -> upstream_cluster.name: Str(envoy_cluster2)
    opentelemetry-opentelemetry-1      |      -> http.status_code: Str(200)
    opentelemetry-opentelemetry-1      |      -> response_flags: Str(-)
    opentelemetry-opentelemetry-1      |    {"kind": "exporter", "data_type": "traces", "name": "debug"}

And the matching access log:

.. code-block:: console
   :linenos:
   :emphasize-lines: 18-19

    opentelemetry-opentelemetry-1      | 2024-05-02T15:08:01.867Z   info    LogsExporter {"kind": "exporter", "data_type": "logs", "name": "debug", "resource logs": 1, "log records": 1}
    opentelemetry-opentelemetry-1      | 2024-05-02T15:08:01.867Z   info    ResourceLog #0
    opentelemetry-opentelemetry-1      | Resource SchemaURL: 
    opentelemetry-opentelemetry-1      | Resource attributes:
    opentelemetry-opentelemetry-1      |      -> log_name: Str(otel_envoy_accesslog)
    opentelemetry-opentelemetry-1      |      -> zone_name: Str()
    opentelemetry-opentelemetry-1      |      -> cluster_name: Str()
    opentelemetry-opentelemetry-1      |      -> node_name: Str()
    opentelemetry-opentelemetry-1      | ScopeLogs #0
    opentelemetry-opentelemetry-1      | ScopeLogs SchemaURL: 
    opentelemetry-opentelemetry-1      | InstrumentationScope  
    opentelemetry-opentelemetry-1      | LogRecord #0
    opentelemetry-opentelemetry-1      | ObservedTimestamp: 1970-01-01 00:00:00 +0000 UTC
    opentelemetry-opentelemetry-1      | Timestamp: 2024-05-02 15:08:01.188307 +0000 UTC
    opentelemetry-opentelemetry-1      | SeverityText: 
    opentelemetry-opentelemetry-1      | SeverityNumber: Unspecified(0)
    opentelemetry-opentelemetry-1      | Body: Empty()
    opentelemetry-opentelemetry-1      | Trace ID: 832518ad68578df8182cd76847693a93
    opentelemetry-opentelemetry-1      | Span ID: 8af519328fd9ec0c
    opentelemetry-opentelemetry-1      | Flags: 0
    opentelemetry-opentelemetry-1      |    {"kind": "exporter", "data_type": "logs", "name": "debug"}

Notice how the log record's ``Trace ID`` and ``Span ID`` fields match the span's ``Trace ID`` and ``ID`` fields.

Step 6: How to replicate this in your setup
*******************************************

The configuration is divided in two parts: setting up the reporting of tracing data over OpenTelemetry, and the reporting of access logs:

Set up a cluster to report to the OpenTelemetry collector (or another endpoint that accepts OTLP v1):

.. literalinclude:: _include/envoy-opentelemetry.yaml
   :language: yaml
   :lines: 104-111
   :linenos:
   :caption: Envoy configuration for a cluster representing an OTLP endpoint

To enable the reporting of tracing data, add the following to the `envoy.filters.network.http_connection_manager` filter: 

.. literalinclude:: _include/envoy-opentelemetry.yaml
   :language: yaml
   :lines: 10-23
   :linenos:
   :emphasize-lines: 5-14
   :caption: Adding the `envoy.tracers.opentelemetry` tracing provider to an existing filter

To enable the reporting of access logs as log records, add the following to the `envoy.filters.network.http_connection_manager` filter:

.. literalinclude:: _include/envoy-opentelemetry.yaml
   :language: yaml
   :lines: 24-73
   :linenos:
   :emphasize-lines: 10
   :caption: Setting up the reporting of access logs to the OTLP cluster

In OpenTelemetry, the `service.name` resource attribute is the single, most important piece of metadata to add to an OpenTelemetry resource.
OpenTelemetry resources are metadata dictionaries to describe which system emits the telemetry.
The resource attributes, i.e., the key-value pairs that specify the resource metadata, are sent along with every piece of telemetry.
The `tracing` provider has a `service_name` field to set up the `service.name` resource attribute:

.. literalinclude:: _include/envoy-opentelemetry.yaml
   :language: yaml
   :lines: 12-21
   :linenos:
   :emphasize-lines: 10
   :caption: Setting the `service.name` resource attribute for tracing data

For access logs, however, you need to set up the `service.name` resource attribute using the `resource_attributes` configuration:

.. literalinclude:: _include/envoy-opentelemetry.yaml
   :language: yaml
   :lines: 24-71
   :linenos:
   :emphasize-lines: 12-16
   :caption: Adding the `service.name` resource attribute to access logs

You should use the same value for the service name between tracing data and access logs.